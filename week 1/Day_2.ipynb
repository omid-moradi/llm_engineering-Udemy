{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYoV53A0Tqx7",
        "outputId": "e9f12580-865b-4cb0-9879-617aadc1342c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DlsqrSjehNV",
        "outputId": "9b63c8d2-91f2-4dc7-b237-c434dd1ba799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4 requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSwS9Iu7T6WO",
        "outputId": "ac3be66c-7761-4f4e-ca72-2b410806c0af"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login --token {\"YOR_TOKEN\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "v_VWIlbIetmM"
      },
      "outputs": [],
      "source": [
        "# ✅ وارد کردن کتابخانه‌ها\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import Markdown, display\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMU6tMLtXcxa",
        "outputId": "7df29f25-b201-41d8-9361-2e1b860f4867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = \"cpu\"\n",
        "if torch.backends.mps.is_available():\n",
        "    DEVICE = \"mps\"\n",
        "elif torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKINWtyGUH4o",
        "outputId": "950a707e-8e7c-4ec1-f428-114fdf84b15d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:897: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# نام مدل\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "# توکنایزر و مدل را بارگذاری می‌کنیم\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,  # استفاده از float16 برای GPU\n",
        "    device_map=\"auto\"           # به صورت خودکار روی GPU یا CPU قرار می‌گیره\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OpEmOJWZUc15"
      },
      "outputs": [],
      "source": [
        "def generate_response(prompt, model, tokenizer, **generation_kwargs):\n",
        "    \"\"\"\n",
        "    Generates a response from the model given a prompt string.\n",
        "    \"\"\"\n",
        "    gen_config = getattr(model, \"generation_config\", GenerationConfig())\n",
        "\n",
        "    default_generation_kwargs = {\n",
        "        \"do_sample\": gen_config.do_sample,\n",
        "        \"top_p\": gen_config.top_p,\n",
        "        \"temperature\": gen_config.temperature,\n",
        "        \"max_new_tokens\": 256\n",
        "    }\n",
        "\n",
        "    generation_kwargs = {**default_generation_kwargs, **generation_kwargs}\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        **generation_kwargs\n",
        "    )\n",
        "\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "    return tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "02DQGmFsV3Lj"
      },
      "outputs": [],
      "source": [
        "def format_messages_as_prompt(messages, tokenizer):\n",
        "    \"\"\"\n",
        "    Converts a list of chat-style messages (with roles) into a formatted prompt\n",
        "    using the tokenizer's chat template (e.g., for LLaMA 3 chat models).\n",
        "    \"\"\"\n",
        "    return tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2DSRA-bRWFyn"
      },
      "outputs": [],
      "source": [
        "# ✅ کلاس Webpage برای گرفتن و تمیز کردن متن وب‌سایت\n",
        "headers = {\n",
        " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "class Website:\n",
        "\n",
        "    def __init__(self, url):\n",
        "        \"\"\"\n",
        "        Create this Website object from the given url using the BeautifulSoup library\n",
        "        \"\"\"\n",
        "        self.url = url\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        self.title = soup.title.string if soup.title else \"No title found\"\n",
        "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
        "            irrelevant.decompose()\n",
        "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAi-3fpLgVkz",
        "outputId": "20c06d8e-405f-4005-f9e3-2bfb4e1f59e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Home - Edward Donner\n",
            "Home\n",
            "Connect Four\n",
            "Outsmart\n",
            "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
            "About\n",
            "Posts\n",
            "Well, hi there.\n",
            "I’m Ed. I like writing code and experimenting with LLMs, and hopefully you’re here because you do too. I also enjoy DJing (but I’m badly out of practice), amateur electronic music production (\n",
            "very\n",
            "amateur) and losing myself in\n",
            "Hacker News\n",
            ", nodding my head sagely to things I only half understand.\n",
            "I’m the co-founder and CTO of\n",
            "Nebula.io\n",
            ". We’re applying AI to a field where it can make a massive, positive impact: helping people discover their potential and pursue their reason for being. Recruiters use our product today to source, understand, engage and manage talent. I’m previously the founder and CEO of AI startup untapt,\n",
            "acquired in 2021\n",
            ".\n",
            "We work with groundbreaking, proprietary LLMs verticalized for talent, we’ve\n",
            "patented\n",
            "our matching model, and our award-winning platform has happy customers and tons of press coverage.\n",
            "Connect\n",
            "with me for more!\n",
            "January 23, 2025\n",
            "LLM Workshop – Hands-on with Agents – resources\n",
            "December 21, 2024\n",
            "Welcome, SuperDataScientists!\n",
            "November 13, 2024\n",
            "Mastering AI and LLM Engineering – Resources\n",
            "October 16, 2024\n",
            "From Software Engineer to AI Data Scientist – resources\n",
            "Navigation\n",
            "Home\n",
            "Connect Four\n",
            "Outsmart\n",
            "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
            "About\n",
            "Posts\n",
            "Get in touch\n",
            "ed [at] edwarddonner [dot] com\n",
            "www.edwarddonner.com\n",
            "Follow me\n",
            "LinkedIn\n",
            "Twitter\n",
            "Facebook\n",
            "Subscribe to newsletter\n",
            "Type your email…\n",
            "Subscribe\n"
          ]
        }
      ],
      "source": [
        "# Let's try one out. Change the website and add print statements to follow along.\n",
        "\n",
        "ed = Website(\"https://edwarddonner.com\")\n",
        "print(ed.title)\n",
        "print(ed.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "PsrPhE5NjD4q",
        "outputId": "199a0891-a62e-4350-d8b8-6f4d4a6793ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "# Home\n",
              "### Overview of Edward Donner's Website\n",
              "\n",
              "Edward Donner's website is a personal and professional portfolio showcasing his work in artificial intelligence, particularly in the realm of Large Language Models (LLMs). The website highlights his involvement with Nebula.io, a company applying AI to talent discovery and management.\n",
              "\n",
              "### About\n",
              "Edward Donner is the co-founder and CTO of Nebula.io, a company that utilizes LLMs to source, understand, engage, and manage talent. He previously founded and served as CEO of AI startup untapt, which was acquired in 2021.\n",
              "\n",
              "### Connect\n",
              "For more information, connect with Edward on various platforms:\n",
              "\n",
              "* LinkedIn\n",
              "* Twitter\n",
              "* Facebook\n",
              "\n",
              "### Resources\n",
              "A collection of resources on AI and LLM engineering, including:\n",
              "\n",
              "* **January 23, 2025:** LLM Workshop – Hands-on with Agents\n",
              "* **December 21, 2024:** Mastering AI and LLM Engineering\n",
              "* **October 16, 2024:** From Software Engineer to AI Data Scientist"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ✅ پیام‌ها با نقش‌های مشخص\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are an assistant that summarizes websites. Respond in markdown, and ignore any navigation text.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"You are looking at a website titled: {ed.title}\\n\\nThe contents of this website is as follows:\\n\\n{ed.text}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# ✅ تبدیل پیام‌ها به prompt نهایی با قالب chat\n",
        "prompt = format_messages_as_prompt(messages, tokenizer)\n",
        "\n",
        "# ✅ تولید پاسخ با مدل\n",
        "summary = generate_response(prompt, model, tokenizer, temperature=0.7)\n",
        "\n",
        "# ✅ نمایش خروجی خلاصه‌شده\n",
        "from IPython.display import Markdown, display\n",
        "display(Markdown(summary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XhgYLHODjGz1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
